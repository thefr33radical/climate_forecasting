{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lstm_lab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "-lJgSqbZ_zfS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## LSTM implementation  for various use cases\n",
        "\n",
        "### Dataset : IMDB movie reviews\n"
      ]
    },
    {
      "metadata": {
        "id": "Ood8lBiDDTE1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "c5aa8ab8-48bf-4424-a952-9ca0e50c685b"
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import LSTM, Embedding, Dense,Flatten\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import sequence\n",
        "\n",
        "\n",
        "def load_data():\n",
        "  \"\"\"\n",
        "  Function to lead data\n",
        "  \"\"\"\n",
        "  (x_train,y_train),(x_test,y_test) = imdb.load_data(num_words=1000)\n",
        "\n",
        "  x_train = sequence.pad_sequences(x_train,maxlen=100)\n",
        "  x_test = sequence.pad_sequences(x_test,maxlen=100)\n",
        "  n_samples = 5000\n",
        "  x_train =x_train[:n_samples]\n",
        "  x_test =x_test[:n_samples]\n",
        "  y_train=y_train[:n_samples]\n",
        "  y_test=y_test[:n_samples]\n",
        "  \n",
        "  return x_train,y_train,x_test,y_test\n",
        "  \n",
        "def compute(x_train,y_train,x_test,y_test):\n",
        "  \"\"\"\n",
        "  Train the Model\n",
        "  \"\"\"\n",
        "  \n",
        "  model = Sequential()\n",
        "  \n",
        "  model.add(Embedding(1000, 10,input_length=100, dropout=0.2, ))\n",
        "  model.add(LSTM(100, dropout=0.5))\n",
        " \n",
        "  model.add(Dense(100, activation='relu'))\n",
        "  model.add(Dense(1, activation='softmax'))\n",
        "  \n",
        "  model.compile(loss='binary_crossentropy',optimizer=\"adam\",metrics=[\"accuracy\"])\n",
        "  model.fit(x_train,y_train,validation_data=(x_test,y_test),batch_size = 2500, epochs=2)\n",
        "  \n",
        "  predict = model.evaluate(x_test, y_test)\n",
        "  print(\"Score : \",predict)\n",
        "  \n",
        "x_train,y_train,x_test,y_test = load_data()\n",
        "compute(x_train,y_train,x_test,y_test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:31: UserWarning: The `dropout` argument is no longer support in `Embedding`. You can apply a `keras.layers.SpatialDropout1D` layer right after the `Embedding` layer to get the same behavior.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 5000 samples, validate on 5000 samples\n",
            "Epoch 1/2\n",
            "5000/5000 [==============================] - 10s 2ms/step - loss: 7.8245 - acc: 0.5092 - val_loss: 8.1976 - val_acc: 0.4858\n",
            "Epoch 2/2\n",
            "5000/5000 [==============================] - 9s 2ms/step - loss: 7.8245 - acc: 0.5092 - val_loss: 8.1976 - val_acc: 0.4858\n",
            "5000/5000 [==============================] - 3s 566us/step\n",
            "Score :  [8.19757472076416, 0.4858]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7GEBXWVbAAvC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Sentiment Analysis"
      ]
    }
  ]
}